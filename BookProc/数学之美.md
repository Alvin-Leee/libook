- 翻译能够达成的原因？
  - 文字只是载体，因为不同的文字系统在记录信息上的能力是等价的
- 信息冗余是信息安全的保障（重复/备份）
- 为什么很多翻译软件和服务都叫“罗塞塔”
  - 因为罗塞塔石碑，它上面的象形文字记录着5000年前古埃及人的生活信息
- 为什么形成了十进制
  - 早期计数使用掰手指头的方法，十根手指头
- 阿拉伯数字（0-9）是由谁传入欧洲的，真正的发明人是谁？
  - 阿拉伯人传入欧洲，所以叫阿拉伯数字，但是真正的发明人是古印度人
- 信息科学中，在通信时，如果信道较宽，不必压缩就可以直接传递；如果信道很窄，在传递前就需要尽可能压缩
- 两人说话块是宽信道，书写慢是窄信道，需要压缩；白话到文言文就是信道压缩；文言文解释清楚就是解压缩
- 古犹太人抄写圣经，每一行和每一列都有校验码
- 计算机高级语言的文法是上下文无关的吗（是）
- 自然语言的文法是上下文无关的吗（不是）
- 计算一个句子的概率P(S) = 条件概率0
- 如何解决单词对没有出现或者出现频率较低的情况（不平滑）
  - 基于大数定理，要求有足够的观测值
  - 在看的见的概率中分一个小的比例给没有看见的事件，至于小多少，要根据“越是不可信的统计折扣越多”的方法进行
- 语料的问题
- 分词的问题
  - 西方拼音语言，有明确的分界符
  - 一些亚洲语言，没有明确的分界符，需要先分句
  - 语言模型：动态规划+维特比算法
  - 不同的应用应该有不同的分词系统，“北京大学”在机器翻译中不能被分成两个词，但在语音识别中，一般是分成两个词
  - 用于中文的分词技术，在英语的手写识别中也派上了用场，因为在书写时单词之间是没有停顿的
  - 针对不同的应用，可以构造不同的分词器，但是是非常浪费的，可以让一个分词器同时支持不同层次的词的切分

- 隐马尔科夫链
  - 语言和通信的联系是密切的（都是编码与解码，以及传输的过程）
  - 如何根据观测信号来推测源信号，就是通过条件概率，得到一个最大值
  - ArgMax Arg是Argument的缩写
  - 将之前的条件概率进一步修改为隐马尔科夫链
- 如何解决围绕隐马尔科夫链的三个问题
  - 给定一个模型，如何计算某个特定的输出序列的概率
    - Forward-Backward算法
  - 给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列
    - 维特比算法
  - 给定足够量的观测数据，如何估计隐马尔科夫模型的参数
    - 如果使用有监督学习，那么是非常耗时耗力的，而且对于一些语音识别中的声学信号是无法确定序列的
    - 无监督学习算法主要有鲍姆-韦尔奇算法（通过观测信号倒退隐马尔科夫模型，可能会有多个适合的模型，该算法就是用来找到最可能的模型）
    - 每一次迭代都是估计新模型的参数，使目标函数（概率）最大化，该过程被称为EM过程，能保证收敛到一个局部最优点，但是不能保证找到全局最优点
    - 所以在某些应用中，无监督学习效果不如有监督学习，但是如果目标函数时凸函数，只有一个最优点，使用EM过程就能找到最佳值

- 信息是如何度量的
  - 香农定义了“信息熵”
  - 32个球队猜冠军，最多需要猜5次（编号猜范围），但是如果把可能性高的分成一组，可能性低的分成另一组，猜的次数就要减少很多
  - 常用汉字7000，若概率都相等，那么需要13bit，若考虑上下文和使用概率，那么每个汉字的信息熵大约只有5比特，那么一本50万字的书，信息熵有250万比特
  - 如果一本书重复的内容很多，信息量就小，冗余度就大
  - 汉字是所有语言中冗余度最小的，比如一本相同内容的书，中文的书很薄
- 信息的不确定性
  - 任何事物内部都有不确定性，为了消除不确定性，就要引入信息量，使用信息来消除一部分不确定性，产生新的不确定性
  - 没有信息，任何公式或数字的游戏都不能消除不确定性
  - 所有自然语言处理的过程，就是一个消除不确定性的过程
  - 熵越大，不确定性就越大
  - 自然语言处理模型中，一元模型就是通过某个词的概率分布，二元及更高级模型使用上下文，就能更准确预测。
  - 并不是消息越多越好，要找相关的消息才能降低不确定性
    - 这就解释了从二元模型引入三元模型时，结果没有发生改变的原因
  - 什么叫互信息
    - 度量两个随机事件的“相关性”
  - 互信息的概率定义公式是什么
  - 互信息的熵定义公式是什么？含义是什么？
    - 在了解Y的前提下，消除另一个X不确定性所提供的信息量
  - 什么是相对熵？KL(f(x) ||  g(x))
    - 衡量两个取值为正数的函数的相似性
    - 相对熵是对称的吗？如何使它对称？
      - KL(f||g) $\ne$ KL(g||f)，取平均
  - TF-IDF是什么算法？（词频率-定向文档频率）
  - 斯坦福大学托马斯科弗（Thomas Cover）是信息论界专家，《信息论基础》



## 布尔代数

- 建立搜索引擎大致要做三件事：
  - 自动下载尽可能多的网页
  - 建立快速有效的索引
  - 根据相关性对网页进行公平准确的排序
- 布尔代数是什么
  - 真1、假0 ==> and、or、not ==> xor
- 在布尔代数的世界中，万物都是可以量子化的
- 今日的搜索引擎要聪明的多，会自动把用户的查询语句转换成布尔运算的算式，然后利用SQL的索引进行数据库搜索
- 比如100篇文献（100位比特），有“数学”关键字的标为1，有“美丽”的标为1，则有“数字”和“美丽”只要对两串做与运算即可
- 根据网页的序号将索引分成很多份，分别存储在许许多多的服务器中，每接受一个查询，就被分发到许多服务器中，并行处理用户请求，结果送到主服务器中进行合并处理，然后返回给用户



## 图论和网络爬虫

- 数理逻辑基于布尔运算
- 如何自动下载互联网所有的网页呢？用图论中的遍历算法
- 网络爬虫就是用的图论遍历原理
- 如何构建一个网络爬虫（常见面试题）
  - 首先，使用BFS还是DFS
    - 如何在有限的时间爬取最多重要的网页
    - BFS爬取首页的链接网页（可能是设计者认为是比较重要的网页）
    - DFS考虑爬虫的分布式以及建立通信的握手成本
    - 选用BFS还是DFS需要有一个相对复杂的优先级排序算法
    - 一般缓存URL存放在一个队列中，所以爬虫使用BFS成分多一些
  - 其次，是分析URL
    - 目前的网页不是简单的HTML，而是有很多JS解析的脚本，所以需要编写解析器来模拟，然后才能得到隐藏的URL
  - 第三，记录曾经访问过的URL
    - 散列表是分布存放的
    - 每个下载服务器在开始下载前和完成下载后都要访问和维护这张表，以免不同的服务器做重复的工作
    - 如何消除爬虫系统中存储散列表服务器间的通信
      - 首先明确每台下载服务器的分工，在调度时就知道URL要交给哪台服务器去下载
      - 然后，判断URL是否下载就可以批处理了，比如每次向散列表（一组服务器）发送一大批询问，或者每次更新一大批散列表的内容，就大大减少了通信次数



### PageRank

---

- 搜索结果的排名取决于两组信息
  - 关于网页的质量信息
  - 这个查询与每个网页的相关性信息
- 民主表决式，指向某个网页的链接数量最多，后续演变成权重最高
- 假设所有网页的排名是相同的，并且根据这个初始值，计算各个网页的第一次迭代排名，然后再计算出第二次迭代排名，自动收敛，不需要人工干预（使用二维矩阵且为稀疏矩阵）
- 简述PageRank的实现原理？及底层矩阵实现原理和稀疏矩阵的平滑实现
- 信息检索课程